{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Do Deep Learning on Graphs with Graph Convolutional Networks\n",
    "## Part 2: Semi-Supervised Learning with Spectral Graph Convolutions\n",
    "This notebook accompanies my Medium article with the above title for readers to try out and explore graph convolutional networks for themselves. You can find the article [here](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0). To run the notebook, install the packages specified in the accompanying ```requirements.txt``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Karate Club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from networkx import read_edgelist, set_node_attributes\n",
    "from pandas import read_csv, Series\n",
    "from numpy import array\n",
    "\n",
    "DataSet = namedtuple(\n",
    "    'DataSet',\n",
    "    field_names=['X_train', 'y_train', 'X_test', 'y_test', 'network']\n",
    ")\n",
    "\n",
    "def load_karate_club():\n",
    "    network = read_edgelist(\n",
    "        'karate.edgelist',\n",
    "        nodetype=int)\n",
    "\n",
    "    attributes = read_csv(\n",
    "        'karate.attributes.csv',\n",
    "        index_col=['node'])\n",
    "\n",
    "    for attribute in attributes.columns.values:\n",
    "        set_node_attributes(\n",
    "            network,\n",
    "            values=Series(\n",
    "                attributes[attribute],\n",
    "                index=attributes.index).to_dict(),\n",
    "            name=attribute\n",
    "        )\n",
    "\n",
    "    X_train, y_train = map(array, zip(*[\n",
    "        ([node], data['role'] == 'Administrator')\n",
    "        for node, data in network.nodes(data=True)\n",
    "        if data['role'] in {'Administrator', 'Instructor'}\n",
    "    ]))\n",
    "    X_test, y_test = map(array, zip(*[\n",
    "        ([node], data['community'] == 'Administrator')\n",
    "        for node, data in network.nodes(data=True)\n",
    "        if data['role'] == 'Member'\n",
    "    ]))\n",
    "    \n",
    "    return DataSet(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import to_numpy_matrix, degree_centrality, betweenness_centrality, shortest_path_length\n",
    "import mxnet.ndarray as nd\n",
    "\n",
    "zkc = load_karate_club()\n",
    "\n",
    "A = to_numpy_matrix(zkc.network)\n",
    "A = nd.array(A)\n",
    "\n",
    "X_train = zkc.X_train.flatten()\n",
    "y_train = zkc.y_train\n",
    "X_test = zkc.X_test.flatten()\n",
    "y_test = zkc.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import HybridBlock\n",
    "from mxnet.gluon.nn import Activation\n",
    "import mxnet.ndarray as nd\n",
    "\n",
    "class SpectralRule(HybridBlock):\n",
    "    def __init__(self, A, in_units, out_units, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        I = nd.eye(*A.shape)\n",
    "        A_hat = A.copy() + I\n",
    "\n",
    "        D = nd.sum(A_hat, axis=0)\n",
    "        D_inv = D**-0.5\n",
    "        D_inv = nd.diag(D_inv)\n",
    "\n",
    "        A_hat = D_inv * A_hat * D_inv\n",
    "        \n",
    "        self.in_units, self.out_units = in_units, out_units\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.A_hat = self.params.get_constant('A_hat', A_hat)\n",
    "            self.W = self.params.get(\n",
    "                'W', shape=(self.in_units, self.out_units)\n",
    "            )\n",
    "            if activation == 'identity':\n",
    "                self.activation = lambda X: X\n",
    "            else:\n",
    "                self.activation = Activation(activation)\n",
    "\n",
    "    def hybrid_forward(self, F, X, A_hat, W):\n",
    "        aggregate = F.dot(A_hat, X)\n",
    "        propagate = self.activation(\n",
    "            F.dot(aggregate, W))\n",
    "        return propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(HybridBlock):\n",
    "    def __init__(self, in_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.w = self.params.get(\n",
    "                'w', shape=(1, in_units)\n",
    "            )\n",
    "\n",
    "            self.b = self.params.get(\n",
    "                'b', shape=(1, 1)\n",
    "            )\n",
    "\n",
    "    def hybrid_forward(self, F, X, w, b):\n",
    "        # Change shape of b to comply with MXnet addition API\n",
    "        b = F.broadcast_axis(b, axis=(0,1), size=(34, 1))\n",
    "        y = F.dot(X, w, transpose_b=True) + b\n",
    "\n",
    "        return F.sigmoid(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.nn import HybridSequential, Activation\n",
    "from mxnet.ndarray import array\n",
    "from mxnet.initializer import One, Uniform, Xavier\n",
    "from mxnet.gluon.loss import SigmoidBinaryCrossEntropyLoss\n",
    "\n",
    "def build_features(A, X):\n",
    "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')] # Format: (units in layer, activation function)\n",
    "    in_units = in_units=X.shape[1]\n",
    "  \n",
    "    features = HybridSequential()\n",
    "    with features.name_scope():\n",
    "        for i, (layer_size, activation_func) in enumerate(hidden_layer_specs):\n",
    "            layer = SpectralRule(\n",
    "                A, in_units=in_units, out_units=layer_size, \n",
    "                activation=activation_func)\n",
    "            features.add(layer)\n",
    "\n",
    "            in_units = layer_size\n",
    "    return features, in_units\n",
    "\n",
    "def build_model(A, X):\n",
    "    model = HybridSequential()\n",
    "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')]\n",
    "    in_units = in_units=X.shape[1]\n",
    "\n",
    "    with model.name_scope():\n",
    "        features, out_units = build_features(A, X)\n",
    "        model.add(features)\n",
    "\n",
    "        classifier = LogisticRegressor(out_units)\n",
    "        model.add(classifier)\n",
    "\n",
    "    model.hybridize()\n",
    "    model.initialize(Uniform(1))\n",
    "\n",
    "    return model, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Identity Matrix as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.5106776 ]\n",
       " [0.5095115 ]\n",
       " [0.51096225]\n",
       " [0.51542187]\n",
       " [0.52302504]\n",
       " [0.512526  ]\n",
       " [0.511652  ]\n",
       " [0.50208426]\n",
       " [0.5050117 ]\n",
       " [0.51883554]\n",
       " [0.5406651 ]\n",
       " [0.49158806]\n",
       " [0.5110878 ]\n",
       " [0.4936631 ]\n",
       " [0.5026252 ]\n",
       " [0.48504698]\n",
       " [0.5104648 ]\n",
       " [0.51194   ]\n",
       " [0.5220448 ]\n",
       " [0.5095117 ]\n",
       " [0.50140274]\n",
       " [0.50985765]\n",
       " [0.5303379 ]\n",
       " [0.51043373]\n",
       " [0.50124794]\n",
       " [0.52045095]\n",
       " [0.50941336]\n",
       " [0.49507195]\n",
       " [0.5021959 ]\n",
       " [0.50682354]\n",
       " [0.5085804 ]\n",
       " [0.51637703]\n",
       " [0.5098626 ]\n",
       " [0.5153003 ]]\n",
       "<NDArray 34x1 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = I = nd.eye(*A.shape)\n",
    "model_1, features_1 = build_model(A, X_1)\n",
    "model_1(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Distance to Administrator and Instructor as Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = nd.zeros((A.shape[0], 2))\n",
    "node_distance_instructor = shortest_path_length(zkc.network, target=33)\n",
    "node_distance_administrator = shortest_path_length(zkc.network, target=0)\n",
    "\n",
    "for node in zkc.network.nodes():\n",
    "    X_2[node][0] = node_distance_administrator[node]\n",
    "    X_2[node][1] = node_distance_instructor[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.45661393]\n",
       " [0.45822805]\n",
       " [0.456899  ]\n",
       " [0.457218  ]\n",
       " [0.45603466]\n",
       " [0.4640951 ]\n",
       " [0.4631094 ]\n",
       " [0.45584652]\n",
       " [0.4608328 ]\n",
       " [0.47084248]\n",
       " [0.4593098 ]\n",
       " [0.4513544 ]\n",
       " [0.46219462]\n",
       " [0.47803804]\n",
       " [0.46955827]\n",
       " [0.49520862]\n",
       " [0.46266046]\n",
       " [0.4590963 ]\n",
       " [0.49145314]\n",
       " [0.46135917]\n",
       " [0.48991072]\n",
       " [0.4575545 ]\n",
       " [0.48936   ]\n",
       " [0.4587967 ]\n",
       " [0.46603325]\n",
       " [0.47440898]\n",
       " [0.48096892]\n",
       " [0.47632974]\n",
       " [0.47681022]\n",
       " [0.47006816]\n",
       " [0.47394764]\n",
       " [0.46878797]\n",
       " [0.4676332 ]\n",
       " [0.49637428]]\n",
       "<NDArray 34x1 @cpu(0)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = nd.concat(X_1, X_2)\n",
    "model_2, features_2 = build_model(A, X_2)\n",
    "model_2(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 20 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import Trainer\n",
    "from mxnet.ndarray import sum as ndsum\n",
    "import numpy as np\n",
    "\n",
    "def train(model, features, X, X_train, y_train, epochs):\n",
    "    cross_entropy = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)\n",
    "    trainer = Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.001, 'momentum': 1})\n",
    "\n",
    "    feature_representations = [features(X).asnumpy()]\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        cum_loss = 0\n",
    "        cum_preds = []\n",
    "\n",
    "        for i, x in enumerate(X_train):\n",
    "            y = array(y_train)[i]\n",
    "            with autograd.record():\n",
    "                preds = model(X)[x]\n",
    "                loss = cross_entropy(preds, y)\n",
    "            loss.backward()\n",
    "            trainer.step(1)\n",
    "\n",
    "            cum_loss += loss.asscalar()\n",
    "            cum_preds += [preds.asscalar()]\n",
    "\n",
    "        feature_representations.append(features(X).asnumpy())\n",
    "            \n",
    "        if (e % (epochs//10)) == 0:\n",
    "            print(f\"Epoch {e}/{epochs} -- Loss: {cum_loss: .4f}\")\n",
    "            print(cum_preds)\n",
    "    return feature_representations\n",
    "\n",
    "def predict(model, X, nodes):\n",
    "    preds = model(X)[nodes].asnumpy().flatten()\n",
    "    return np.where(preds >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/5000 -- Loss:  0.0001\n",
      "[0.9999902, 4.623503e-05]\n",
      "Epoch 1000/5000 -- Loss:  0.0000\n",
      "[1.0, 8.968348e-10]\n",
      "Epoch 1500/5000 -- Loss:  0.0000\n",
      "[1.0, 1.7521857e-14]\n",
      "Epoch 2000/5000 -- Loss:  0.0000\n",
      "[1.0, 3.429584e-19]\n",
      "Epoch 2500/5000 -- Loss:  0.0000\n",
      "[1.0, 6.7126565e-24]\n",
      "Epoch 3000/5000 -- Loss:  0.0000\n",
      "[1.0, 1.313875e-28]\n",
      "Epoch 3500/5000 -- Loss:  0.0000\n",
      "[1.0, 2.5716606e-33]\n",
      "Epoch 4000/5000 -- Loss:  0.0000\n",
      "[1.0, 5.060105e-38]\n",
      "Epoch 4500/5000 -- Loss:  0.0000\n",
      "[1.0, 0.0]\n",
      "Epoch 5000/5000 -- Loss:  0.0000\n",
      "[1.0, 0.0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.62      0.50      0.55        16\n",
      "        True       0.58      0.69      0.63        16\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        32\n",
      "   macro avg       0.60      0.59      0.59        32\n",
      "weighted avg       0.60      0.59      0.59        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "feature_representations_1 = train(model_1, features_1, X_1, X_train, y_train, epochs=5000)\n",
    "y_pred_1 = predict(model_1, X_1, X_test)\n",
    "print(classification_report(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250 -- Loss:  1.4502\n",
      "[0.46630818, 0.49708325]\n",
      "Epoch 50/250 -- Loss:  1.4058\n",
      "[0.48751295, 0.49711376]\n",
      "Epoch 75/250 -- Loss:  1.3709\n",
      "[0.5132726, 0.5053706]\n",
      "Epoch 100/250 -- Loss:  1.3566\n",
      "[0.5332241, 0.5170041]\n",
      "Epoch 125/250 -- Loss:  1.3161\n",
      "[0.5386774, 0.5021284]\n",
      "Epoch 150/250 -- Loss:  1.1604\n",
      "[0.5339666, 0.41315272]\n",
      "Epoch 175/250 -- Loss:  0.8730\n",
      "[0.5482581, 0.23811255]\n",
      "Epoch 200/250 -- Loss:  0.5649\n",
      "[0.625712, 0.09159255]\n",
      "Epoch 225/250 -- Loss:  0.3007\n",
      "[0.7656582, 0.033075843]\n",
      "Epoch 250/250 -- Loss:  0.1270\n",
      "[0.89338917, 0.014171573]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.94      0.88        16\n",
      "        True       0.93      0.81      0.87        16\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        32\n",
      "   macro avg       0.88      0.88      0.87        32\n",
      "weighted avg       0.88      0.88      0.87        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_representations_2= train(model_2, features_2, X_2, X_train, y_train, epochs=250)\n",
    "y_pred_2 = predict(model_2, X_2, X_test)\n",
    "print(classification_report(y_test, y_pred_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posts",
   "language": "python",
   "name": "posts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
